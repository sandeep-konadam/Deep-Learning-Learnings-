{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMqKJEfGhcfJw8TeGEH43Nm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FB5iTo4dUbWG","executionInfo":{"status":"ok","timestamp":1699425507680,"user_tz":-330,"elapsed":16647,"user":{"displayName":"Konadam Sandeep","userId":"06472152593849570626"}},"outputId":"be517a49-663a-455f-8420-1e3cdca8cbdd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n","  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers<0.15,>=0.14 (from transformers)\n","  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n","  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n","Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.35.0\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","source":["import pandas as pd\n","data = pd.read_csv(\"Book4.csv\",encoding='ISO-8859-1')\n","import numpy as np"],"metadata":{"id":"UuFFUlMtp1Dj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.head"],"metadata":{"id":"Rf9AXgR0p7xa","executionInfo":{"status":"ok","timestamp":1699437807934,"user_tz":-330,"elapsed":456,"user":{"displayName":"Konadam Sandeep","userId":"06472152593849570626"}},"outputId":"e8c419f2-49cd-4a52-cfa6-0a3d7170e41f","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<bound method NDFrame.head of                       Text  Sentiment\n","0                 Sooo SAD          0\n","1              bullying me          0\n","2           leave me alone          0\n","3            Sons of ****,          0\n","4                      fun          1\n","...                    ...        ...\n","3999           Sleep-fail.          0\n","4000  What a miserable day          0\n","4001                gutted          0\n","4002                Thanks          1\n","4003                sucks.          0\n","\n","[4004 rows x 2 columns]>"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","\n","encoded_data = tokenizer.batch_encode_plus(\n","    data[\"Text\"].tolist(),\n","    add_special_tokens=True,  # Add [CLS] and [SEP] tokens\n","    padding=\"max_length\",\n","    max_length=128,\n","    truncation=True,\n","    return_attention_mask=True,\n","    return_tensors=\"pt\"  # Return PyTorch tensors\n",")\n"],"metadata":{"id":"mOxNjflKp2Db"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","X = encoded_data[\"input_ids\"]\n","y = data[\"Sentiment\"]\n","\n","X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n","X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n"],"metadata":{"id":"AoaOLs_5sMlO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(type(X_train))\n","print(type(y_train))\n","print(type(X_val))\n","print(type(y_val))\n","print(type(X_test))\n","print(type(y_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sL3QM-UJsTPN","executionInfo":{"status":"ok","timestamp":1699437817743,"user_tz":-330,"elapsed":701,"user":{"displayName":"Konadam Sandeep","userId":"06472152593849570626"}},"outputId":"53eee9c2-7eb3-4c22-e04f-2f56f85ae376"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'torch.Tensor'>\n","<class 'pandas.core.series.Series'>\n","<class 'torch.Tensor'>\n","<class 'pandas.core.series.Series'>\n","<class 'torch.Tensor'>\n","<class 'pandas.core.series.Series'>\n"]}]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","y_train = torch.tensor(y_train.values, dtype=torch.long)\n","y_val = torch.tensor(y_val.values, dtype=torch.long)\n","y_test = torch.tensor(y_test.values, dtype=torch.long)\n","\n","train_dataset = TensorDataset(X_train, y_train)\n","train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","\n","val_dataset = TensorDataset(X_val, y_val)\n","val_dataloader = DataLoader(val_dataset, batch_size=32)\n","\n","test_dataset = TensorDataset(X_test, y_test)\n","test_dataloader = DataLoader(test_dataset, batch_size=32)\n"],"metadata":{"id":"PtlPVEoRsW6N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertForSequenceClassification, AdamW\n","from transformers import get_linear_schedule_with_warmup\n","\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n","\n","optimizer = AdamW(model.parameters(), lr=1e-5)\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader))\n","epochs=3\n","\n","for epoch in range(epochs):\n","    model.train()\n","    for batch in train_dataloader:\n","        inputs, labels = batch\n","        outputs = model(inputs, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","        optimizer.zero_grad()\n","\n","model.eval()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iOxxnMNKsaYX","outputId":"3c3d4cb2-2088-4a73-a2b4-cd4bb03b26f9","executionInfo":{"status":"ok","timestamp":1699449095935,"user_tz":-330,"elapsed":11246354,"user":{"displayName":"Konadam Sandeep","userId":"06472152593849570626"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["# Save the fine-tuned model\n","model.save_pretrained(\"model.h5\")"],"metadata":{"id":"ifGgrd_QYDCP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertForSequenceClassification\n","\n","# Load the saved model\n","model = BertForSequenceClassification.from_pretrained(\"model.h5\")\n"],"metadata":{"id":"cZ_uDC3fYXqU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CIsQ6WCyYb2K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.eval()\n","\n","new_data = [\"love \", \"negative\"]\n","inputs = tokenizer(new_data, truncation=True, padding=True, return_tensors=\"pt\")\n","\n","\n","with torch.no_grad():\n","    outputs = model(**inputs)\n","\n","# Get predictions (0 for negative, 1 for positive)\n","predictions = torch.argmax(outputs.logits, dim=1).tolist()\n","print(predictions)\n","\n","for text, prediction in zip(new_data, predictions):\n","    print(f\"Text: {text}\")\n","    print(f\"Sentiment Prediction: {'Positive' if prediction == 1 else 'Negative'}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T15t1RBGXYFa","executionInfo":{"status":"ok","timestamp":1699449503151,"user_tz":-330,"elapsed":708,"user":{"displayName":"Konadam Sandeep","userId":"06472152593849570626"}},"outputId":"f776330e-ab87-413b-83d3-9f659611ae28"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0, 0]\n","Text: love \n","Sentiment Prediction: Negative\n","Text: negative\n","Sentiment Prediction: Negative\n"]}]}]}